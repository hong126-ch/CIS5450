{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hong126-ch/CIS5450/blob/main/9_Module_2_Notebook_V_Big_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cy6hpgnJY-Rp"
   },
   "source": [
    "# Big Data and Graph Data\n",
    "\n",
    "In this module, we'll take what we learned about indices and generalize!\n",
    "\n",
    "Apache Spark is a big data engine that runs on compute clusters, including on the cloud.  This notebook is set up assuming that (1) Spark is running on an AWS server that is public [this may **not** be true at the time you look at this!].\n",
    "\n",
    "You may need to look at this notebook without directly running it, until we give you specific instructions on launching your own Spark cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5uxG6qfFwMI"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: EMR_HOST=ec2-52-23-243-142.compute-1.amazonaws.com\n",
      "env: HW_ID=cis5450_25f_HW9\n"
     ]
    }
   ],
   "source": [
    "# TODO: fill this one in based on the host posted on Ed\n",
    "%set_env EMR_HOST = ec2-52-23-243-142.compute-1.amazonaws.com\n",
    "%set_env HW_ID=cis5450_25f_HW9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Start Spark Session by Specifying the Spark Cluster Address.\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"CIS-5450\") \\\n",
    "  .remote(\"sc://{host}\".format(host=os.getenv('EMR_HOST'))).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stbF4ttIQBUG"
   },
   "source": [
    "The following line connects to Spark running remotely (note you'll need to start an Amazon AWS Elastic MapReduce instance)\n",
    ".  You will likely need to change the URL after the `-u` to connect to an active server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7pUp7HAwQp6"
   },
   "source": [
    "## Autograder setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
    "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
    "STUDENT_ID = 64660501 # YOUR PENN-ID GOES HERE AS AN INTEGER##PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing notebook-config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile notebook-config.yaml\n",
    "\n",
    "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
    "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting penngrader-client\n",
      "  Downloading penngrader_client-0.5.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from penngrader-client) (0.3.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from penngrader-client) (6.0.3)\n",
      "Downloading penngrader_client-0.5.2-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: penngrader-client\n",
      "Successfully installed penngrader-client-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install penngrader-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PennGrader initialized with Student ID: 64660501\n",
      "\n",
      "Make sure this correct or we will not be able to store your grade\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from penngrader.grader import *\n",
    "\n",
    "grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, STUDENT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6u6rwG-wQIX2"
   },
   "source": [
    "## Example of Loading Sharded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8O02TOXqQLgt"
   },
   "source": [
    "First let's do our preliminaries.  **Every** cell in this notebook will need `%%spark` at the start so it runs on the remote machine with Spark on it, instead of on the machine with Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El-Or-F-Qc5C"
   },
   "source": [
    "## Load into Spark\n",
    "\n",
    "Spark needs to know the structure of the data in its dataframes, i.e., their schemas.  Over the years it has gotten better at inferring schemas, but sometimes you'll want to set the schema yourself.\n",
    "\n",
    "There are some basic types:\n",
    "  * The table is a `StructType` with a list of fields (each row)\n",
    "  * Most fields, in our case, are `StringType`.\n",
    "  * We also have nested dictionary for the name, which is a `MapType` from `StringType` keys to `StringType` values.\n",
    "  * `skills` is an `ArrayType` since it's a list, and it contains `StringType`s.\n",
    "  * `also_view` is an array of structs.\n",
    "\n",
    "See Pyspark documentation on `StructType` and examples such as https://www.programcreek.com/python/example/104715/pyspark.sql.types.StructType.\n",
    "\n",
    "See below for a partial sketch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark uses schemas to define the format for DataFrames. By default it will\n",
    "# try to infer, which has varying luck. Here is an example of part of a schema\n",
    "# for LinkedIn.\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, MapType\n",
    "schema = StructType([\n",
    "        StructField(\"_id\", StringType(), True),\n",
    "        StructField(\"name\", MapType(StringType(), StringType()), True),\n",
    "        StructField(\"locality\", StringType(), True),\n",
    "        StructField(\"skills\", ArrayType(StringType()), True),\n",
    "        StructField(\"industry\", StringType(), True),\n",
    "        StructField(\"summary\", StringType(), True),\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"also_view\", ArrayType(\\\n",
    "                    StructType([\\\n",
    "                      StructField(\"url\", StringType(), True),\\\n",
    "                      StructField(\"id\", StringType(), True)])\\\n",
    "                    ), True)\\\n",
    "         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nB1yjKEpvOC"
   },
   "source": [
    "Let's now load a remote file.  To do this, we add the URL to the sparkContext, and then (in the next Cell) we will use `spark.read.json` to open and load the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                _id|           education|              events|          experience|               group|              honors|            industry|           interests|interval|            locality|                name|              skills|         specilities|             summary|                 url|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|exhaustive-diatonic|[{NULL, , 1990, N...|[{23873, taller t...|[{Empresa dedicad...|{[ATEB - ASOCIACI...|[2004-2006, Vicep...|        Construcci√≥n|                NULL|    28.0|Barcelona y alred...|   {Douglas, Alfred}|[AutoCAD, MS Proj...|                NULL|Gerente de Cister...|https://www.linke...|\n",
      "|  primordial-flight|[{Bachelor of Art...|                  []|[{I am still acti...|{[Realty Services...|[Chicago Apartmen...|        Online Media|business, investm...|     0.0|San Leandro, Cali...|  {Russell, Barkley}|[Negotiation, Res...|contract negotiat...|Sales, Management...|https://www.linke...|\n",
      "|    absolute-bazaar|                NULL|                  []|[{In charge of Pr...|                NULL|                NULL|   Computer Software|                NULL|     0.0|Vancouver, Canada...| {Ogilvy, Leporello}|[Executive Manage...|                NULL|                NULL|https://www.linke...|\n",
      "|      proud-trainer|[{Level 5 - Proce...|                  []|[{Decision Suport...|{[Antigos Alunos ...|                NULL|Tecnologia da inf...|                NULL|     0.0|Lisbon Area, Port...|  {Lennox, Merriman}|                NULL|                NULL|- Microsoft SQL S...|https://www.linke...|\n",
      "|          flat-beef|[{NULL, , 1995, N...|[{24018, Mediaedg...|[{NULL, Present, ...|                NULL|                NULL|Marketing and Adv...|                NULL|    45.0|New South Wales, ...|{Charteris, Barry...|[Media Planning, ...|                NULL|                NULL|https://www.linke...|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read JSON Lines file\n",
    "linked_df = spark.read\\\n",
    "  .json(\"s3://penn-cis545-files/linkedin_anon.jsonl\")\\\n",
    "  .repartition('_id')\n",
    "\n",
    "linked_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v3omsR8p3oQ"
   },
   "source": [
    "We can see the full, inferred schema here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- education: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- degree: string (nullable = true)\n",
      " |    |    |-- desc: string (nullable = true)\n",
      " |    |    |-- end: string (nullable = true)\n",
      " |    |    |-- major: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- start: string (nullable = true)\n",
      " |-- events: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- end: long (nullable = true)\n",
      " |    |    |-- from: string (nullable = true)\n",
      " |    |    |-- start: long (nullable = true)\n",
      " |    |    |-- title1: string (nullable = true)\n",
      " |    |    |-- title2: string (nullable = true)\n",
      " |    |    |-- to: string (nullable = true)\n",
      " |-- experience: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- desc: string (nullable = true)\n",
      " |    |    |-- end: string (nullable = true)\n",
      " |    |    |-- org: string (nullable = true)\n",
      " |    |    |-- start: string (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |-- group: struct (nullable = true)\n",
      " |    |-- affilition: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- member: string (nullable = true)\n",
      " |-- honors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- interests: string (nullable = true)\n",
      " |-- interval: double (nullable = true)\n",
      " |-- locality: string (nullable = true)\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- family_name: string (nullable = true)\n",
      " |    |-- given_name: string (nullable = true)\n",
      " |-- skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- specilities: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linked_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g74x035p-rz"
   },
   "source": [
    "Let's try a simple select/project query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-------------+\n",
      "|               _id|               name|     locality|\n",
      "+------------------+-------------------+-------------+\n",
      "|        claret-dog| {Glengarry, Poole}|United States|\n",
      "|     ternary-model|   {Graham, Figaro}|United States|\n",
      "|searing-brownstone| {Haldane, Brunton}|United States|\n",
      "|         ecru-fire|{Charteris, Alfred}|United States|\n",
      "|       quiet-glide|{Wallace, Merriman}|United States|\n",
      "+------------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linked_df.filter(linked_df.locality == 'United States')[['_id', 'name', 'locality']].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pl3Y2yQjqBCY"
   },
   "source": [
    "Also in SQL-like syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+\n",
      "|              _id|                name|            locality|\n",
      "+-----------------+--------------------+--------------------+\n",
      "|    matching-form|{Sterling, Merriman}|               Malta|\n",
      "|     tender-river|  {Rollo, Bullimore}|Greater New York ...|\n",
      "|fermented-vehicle|{Livingstone, Cad...|Greater Chicago Area|\n",
      "|  overcast-vector|    {Ramsay, Figaro}|Brussels Area, Be...|\n",
      "|  mean-projectile|{Kennedy, Barrymore}|London, Greater L...|\n",
      "+-----------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linked_df.select(\"_id\", 'name', \"locality\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhpz-wUSqGXO"
   },
   "source": [
    "And real SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                _id|           education|              events|          experience|               group|              honors|            industry|           interests|interval|            locality|                name|              skills|         specilities|             summary|                 url|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|exhaustive-diatonic|[{NULL, , 1990, N...|[{23873, taller t...|[{Empresa dedicad...|{[ATEB - ASOCIACI...|[2004-2006, Vicep...|        Construcci√≥n|                NULL|    28.0|Barcelona y alred...|   {Douglas, Alfred}|[AutoCAD, MS Proj...|                NULL|Gerente de Cister...|https://www.linke...|\n",
      "|  primordial-flight|[{Bachelor of Art...|                  []|[{I am still acti...|{[Realty Services...|[Chicago Apartmen...|        Online Media|business, investm...|     0.0|San Leandro, Cali...|  {Russell, Barkley}|[Negotiation, Res...|contract negotiat...|Sales, Management...|https://www.linke...|\n",
      "|    absolute-bazaar|                NULL|                  []|[{In charge of Pr...|                NULL|                NULL|   Computer Software|                NULL|     0.0|Vancouver, Canada...| {Ogilvy, Leporello}|[Executive Manage...|                NULL|                NULL|https://www.linke...|\n",
      "|      proud-trainer|[{Level 5 - Proce...|                  []|[{Decision Suport...|{[Antigos Alunos ...|                NULL|Tecnologia da inf...|                NULL|     0.0|Lisbon Area, Port...|  {Lennox, Merriman}|                NULL|                NULL|- Microsoft SQL S...|https://www.linke...|\n",
      "|          flat-beef|[{NULL, , 1995, N...|[{24018, Mediaedg...|[{NULL, Present, ...|                NULL|                NULL|Marketing and Adv...|                NULL|    45.0|New South Wales, ...|{Charteris, Barry...|[Media Planning, ...|                NULL|                NULL|https://www.linke...|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linked_df.createOrReplaceTempView('linked_in')\n",
    "spark.sql('select * from linked_in').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-----------+\n",
      "|              _id|given_name|family_name|\n",
      "+-----------------+----------+-----------+\n",
      "|    matching-form|  Merriman|   Sterling|\n",
      "|     tender-river| Bullimore|      Rollo|\n",
      "|fermented-vehicle|   Cadbury|Livingstone|\n",
      "|  overcast-vector|    Figaro|     Ramsay|\n",
      "|  mean-projectile| Barrymore|    Kennedy|\n",
      "+-----------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select _id, name.given_name, name.family_name from linked_in\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-Y3apDpePsd"
   },
   "source": [
    "This currently (Fall 2025) does not work between Colab and Apache Spark on Amazon Elastic Mapreduce, simply because Amazon EMR is 3 versions behind in Python (3.9 vs 3.12). When an updated EMR is available, it will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@udf(returnType=StringType(), useArrow=True)\n",
    "def acro(x: str):\n",
    "    return ''.join([n[0] for n in x.split()])\n",
    "\n",
    "# linked_df.select(\"_id\", acro(\"locality\").alias(\"acronym\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|count(_id)|            industry|\n",
      "+----------+--------------------+\n",
      "|      5437|Information Techn...|\n",
      "|      3396|   Computer Software|\n",
      "|      2571|Marketing and Adv...|\n",
      "|      2089|            Internet|\n",
      "|      1485|  Financial Services|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which industries are most popular?\n",
    "spark.sql('select count(_id), industry '+\\\n",
    "               'from linked_in '+\\\n",
    "               'group by industry '+\\\n",
    "               'order by count(_id) desc').\\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRrsSWsmI7pG"
   },
   "source": [
    "## Graphs\n",
    "\n",
    "For the next set of examples, we will look at graph-structured data.  It turns out our LinkedIn dataset has a list of nodes (by int ID, but associated with the user ID we used in the linked_in table) and a list of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|            from|                  to|\n",
      "+----------------+--------------------+\n",
      "|  cheerful-anode|Nikiel & Zacharze...|\n",
      "|  tender-paprika|                 UGT|\n",
      "| absolute-prison|University of Mic...|\n",
      "|  interior-board|Enterprise Inform...|\n",
      "|pleasant-skipper|HAN / ROC RijnIJs...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's consider edges to be bidirectional\n",
    "# from people and the organizations they work for\n",
    "temp_df = spark.sql(\"\"\"\n",
    "  CREATE TEMPORARY VIEW edges_nested AS\n",
    "  SELECT _id AS from, explode(experience) AS to\n",
    "  FROM linked_in\n",
    "\"\"\")\n",
    "\n",
    "# Create graph with edges in each direction\n",
    "edges_df = spark.sql('''\n",
    "  select from, to.org as to from edges_nested\n",
    "  union\n",
    "  select to.org as from, from as to from edges_nested\n",
    "  ''')\n",
    "\n",
    "edges_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from: string (nullable = true)\n",
      " |-- to: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|                 id|degree|\n",
      "+-------------------+------+\n",
      "|               NULL|    73|\n",
      "|      ash-euphemism|     4|\n",
      "|     moderato-phase|     4|\n",
      "|     fried-olympics|    13|\n",
      "|exhaustive-diatonic|    10|\n",
      "+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df.createOrReplaceTempView('edges')\n",
    "spark.sql('select from as id, count(to) as degree from edges group by from').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbXkhbzOk1ZK"
   },
   "source": [
    "## Traversing the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 start nodes\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  .|\n",
      "| 23|\n",
      "| 26|\n",
      "|636|\n",
      "|  3|\n",
      "|  1|\n",
      "|212|\n",
      "|.99|\n",
      "|513|\n",
      "+---+\n",
      "\n",
      "+-------------+\n",
      "|           id|\n",
      "+-------------+\n",
      "|  median-helo|\n",
      "| offline-mint|\n",
      "|crunchy-claim|\n",
      "|    sunny-ayu|\n",
      "| clear-period|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Start with a subset of nodes, looking at everything\n",
    "# that could be considered a number under 1000\n",
    "start_nodes_df = edges_df[['from']].filter(edges_df['from'] < 1000).\\\n",
    "  select(col('from').alias('id')).drop_duplicates()\n",
    "\n",
    "print('{} start nodes'.format(start_nodes_df.count()))\n",
    "start_nodes_df.show(9)\n",
    "\n",
    "# The neighbors require us to join\n",
    "# and we'll use Spark DataFrames syntax here\n",
    "neighbor_nodes_df = start_nodes_df.\\\n",
    "  join(edges_df.alias('e'), start_nodes_df.id == col('e.from')).\\\n",
    "  select(col('to').alias('id'))\n",
    "\n",
    "neighbor_nodes_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                from|\n",
      "+--------------------+\n",
      "|                NULL|\n",
      "|Kj√∏benhavns Boldklub|\n",
      "|MARICO INDUSTRIES...|\n",
      "|      ArtIstanbul PR|\n",
      "|       doughy-format|\n",
      "|brute-force-instance|\n",
      "|       ash-euphemism|\n",
      "|         glass-grove|\n",
      "|      Further Afield|\n",
      "|Orchestre symphon...|\n",
      "|B.I.T. Mesra, Ranchi|\n",
      "|          Expy, Inc.|\n",
      "|        daring-layer|\n",
      "|       boolean-triad|\n",
      "|             icy-tin|\n",
      "|       humane-script|\n",
      "|      careful-height|\n",
      "|       either-symbol|\n",
      "|         finite-tint|\n",
      "|      rancid-cobbler|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+------------+\n",
      "|from|          to|\n",
      "+----+------------+\n",
      "|   1|nippy-folder|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df[['from']].orderBy('from').drop_duplicates().show()\n",
    "\n",
    "edges_df.filter(edges_df['from'] == '1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  id|\n",
      "+--------------------+\n",
      "|                   .|\n",
      "|   Advus Corporation|\n",
      "|Grupo Santander B...|\n",
      "|     Photon Infotech|\n",
      "|      Reino Aventura|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neighbor_neighbor_nodes_df = neighbor_nodes_df.\\\n",
    "  join(edges_df.alias('e'), neighbor_nodes_df.id == col('e.from')).\\\n",
    "  select(col('to').alias('id'))\n",
    "\n",
    "neighbor_neighbor_nodes_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-esxdcgYqsHt"
   },
   "source": [
    "Let's find a small subset of our graph that actually connects somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+--------------------+\n",
      "|from|                med|                  to|\n",
      "+----+-------------------+--------------------+\n",
      "|   .|matching-commission|                   .|\n",
      "|   .|       greasy-chain|   Advus Corporation|\n",
      "|   .|       greasy-chain|Grupo Santander B...|\n",
      "|   .|      light-entropy|     Photon Infotech|\n",
      "|  23|     champagne-grid|      Reino Aventura|\n",
      "+----+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+---------------+\n",
      "|from|             to|\n",
      "+----+---------------+\n",
      "|   .|sunny-interface|\n",
      "|   .|    median-helo|\n",
      "|   .|   offline-mint|\n",
      "|   .|  crunchy-claim|\n",
      "|   .|      sunny-ayu|\n",
      "+----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_nodes_df.createOrReplaceTempView('start_nodes')\n",
    "edges_df.createOrReplaceTempView('edges')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  select e1.from as from, e1.to as med, e2.to as to\n",
    "  from start_nodes s join edges e1 on s.id=e1.from join edges e2 on e1.to = e2.from\n",
    "\"\"\").show(5)\n",
    "\n",
    "# This will be the starting\n",
    "start_df = spark.sql(\"\"\"\n",
    "  select e1.from as from, e1.to as to\n",
    "  from start_nodes s join edges e1 on s.id=e1.from\n",
    "\"\"\")\n",
    "\n",
    "start_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(df, edges, depth):\n",
    "  df.createOrReplaceTempView('base')\n",
    "  edges.createOrReplaceTempView('iter')\n",
    "\n",
    "  # Base case: direct connection\n",
    "  result = spark.sql('select from, to, 1 as depth from base')\n",
    "\n",
    "  for i in range(1, depth):\n",
    "    result.createOrReplaceTempView('result')\n",
    "    result = spark.sql(\"\"\"select r1.from as from, r2.to as to, r1.depth+1 as depth\n",
    "                            from result r1 join iter r2\n",
    "                            on r1.to=r2.from\n",
    "                            where r1.from <> r2.to\n",
    "                            \"\"\")\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-----+\n",
      "|from|                 to|depth|\n",
      "+----+-------------------+-----+\n",
      "|   .|aquamarine-fortress|    1|\n",
      "|   .|     avocado-umpire|    1|\n",
      "|   .|    blaring-trainer|    1|\n",
      "|   .|         canary-bow|    1|\n",
      "|   .|     chestnut-liner|    1|\n",
      "|   .|        citric-byte|    1|\n",
      "|   .|       clear-period|    1|\n",
      "|   .|      crunchy-claim|    1|\n",
      "|   .|        devout-harp|    1|\n",
      "|   .|       direct-force|    1|\n",
      "|   .|          dry-fixed|    1|\n",
      "|   .|       greasy-chain|    1|\n",
      "|   .|        humid-lodge|    1|\n",
      "|   .|     isochoric-drum|    1|\n",
      "|   .|      light-entropy|    1|\n",
      "|   .|matching-commission|    1|\n",
      "|   .|        median-helo|    1|\n",
      "|   .|     mild-animation|    1|\n",
      "|   .|      noisy-diction|    1|\n",
      "|   .|       offline-mint|    1|\n",
      "+----+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iterate(start_df, edges_df, 1).orderBy('from','to').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+\n",
      "|from|                  to|depth|\n",
      "+----+--------------------+-----+\n",
      "|   .|ABRA Enterprises,...|    2|\n",
      "|   .|       AT IT Limited|    2|\n",
      "|   .|Advanced Systems ...|    2|\n",
      "|   .|   Advus Corporation|    2|\n",
      "|   .|Airtours Internat...|    2|\n",
      "|   .|Airtours Internat...|    2|\n",
      "|   .|Airtours plc seco...|    2|\n",
      "|   .|         Alinma Bank|    2|\n",
      "|   .|     Aspen Insurance|    2|\n",
      "|   .| BAE Defense Systems|    2|\n",
      "|   .|    Baan Development|    2|\n",
      "|   .|          Baan/Xebic|    2|\n",
      "|   .|    Bayer Healthcare|    2|\n",
      "|   .|     Belastingdienst|    2|\n",
      "|   .|             Betfair|    2|\n",
      "|   .|CASE Communicatio...|    2|\n",
      "|   .|CableCom Networki...|    2|\n",
      "|   .|             Camelot|    2|\n",
      "|   .|Canadian Leisure ...|    2|\n",
      "|   .|   Canon New Zealand|    2|\n",
      "+----+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iterate(start_df, edges_df, 2).orderBy('from','to').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-----+\n",
      "|from|                 to|depth|\n",
      "+----+-------------------+-----+\n",
      "|   .|  absolute-cabernet|    3|\n",
      "|   .|      absolute-fort|    3|\n",
      "|   .| accepting-ancestor|    3|\n",
      "|   .|  achromatic-panini|    3|\n",
      "|   .|  achromatic-pepato|    3|\n",
      "|   .|  achromatic-pepato|    3|\n",
      "|   .|achromatic-revolver|    3|\n",
      "|   .|  achromatic-upload|    3|\n",
      "|   .|       acidic-asset|    3|\n",
      "|   .|       acidic-delta|    3|\n",
      "|   .|acoustic-rottweiler|    3|\n",
      "|   .|      active-energy|    3|\n",
      "|   .|       active-event|    3|\n",
      "|   .|       active-liner|    3|\n",
      "|   .|        active-roof|    3|\n",
      "|   .|    acute-amplifier|    3|\n",
      "|   .|      acute-halibut|    3|\n",
      "|   .|       acyclic-halo|    3|\n",
      "|   .|  adaptive-bisector|    3|\n",
      "|   .|adaptive-gorgonzola|    3|\n",
      "+----+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iterate(start_df, edges_df, 3).orderBy('from','to').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akCba0mxkikw"
   },
   "source": [
    "## Joins in Spark, Beyond Graph Traversals\n",
    "\n",
    "\n",
    "What if we want to look at relationships between people -- say, co-working?  This involves looking at people and going *every 2 hops* because there are organizations in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecVwMCIspVPc"
   },
   "source": [
    "## Finding Coworkers, by ID\n",
    "\n",
    "Let's get our people first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = spark.sql(\"\"\"\n",
    "    select _id as nid, concat(name.given_name, ' ', name.family_name) as user, industry\n",
    "    from linked_in\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df.createOrReplaceTempView('nodes')\n",
    "\n",
    "# Let's limit coworkers to edges that start\n",
    "# from existing workers, and are 2 hops away (through a company)\n",
    "coworked_df = spark.sql(\"\"\"\n",
    "  select e1.from, e2.to as to\n",
    "  from edges e1 join edges e2 on e1.to = e2.from\n",
    "  where e1.from in (select nid from nodes)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|              user|           coworker|\n",
      "+------------------+-------------------+\n",
      "|     Brunton Munro|Merriman Sutherland|\n",
      "|Bullimore Sinclair|Merriman Sutherland|\n",
      "|      Figaro Creel|Merriman Sutherland|\n",
      "|    Leporello Greg|Merriman Sutherland|\n",
      "|  Barrymore Fraser|Merriman Sutherland|\n",
      "+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes_df.createOrReplaceTempView('nodes')\n",
    "coworked_df.createOrReplaceTempView('edges')\n",
    "\n",
    "coworkers_df = spark.sql(\"\"\"SELECT n1.user, n2.user as coworker\n",
    "               FROM (nodes n1 join edges e on n1.nid = e.from) join nodes n2 on e.to = n2.nid\n",
    "               WHERE n1.user <> n2.user\n",
    "               \"\"\")\n",
    "\n",
    "coworkers_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZCvjQKHtT7p"
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Can you find the *company* with the most common coworker pairs?\n",
    "\n",
    "As a starting point, let's pull back the original edges table..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df.createOrReplaceTempView(\"edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRAC14bdSOHj"
   },
   "source": [
    "Be sure to find the top-1 organization by count (you should return the `org` and the `count` in the schema).\n",
    "\n",
    "Recall that SQL has `ORDER BY` and `LIMIT` clauses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISIzrRTcde05"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|org| count|\n",
      "+---+------+\n",
      "|IBM|272894|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: create coworkers_company_sdf (Spark DataFrame).\n",
    "# You don't need to convert to Pandas\n",
    "coworkers_company_sdf = spark.sql(\"\"\"SELECT e1.to as org, COUNT(*) as count\n",
    "    FROM edges e1\n",
    "    JOIN edges e2 ON e1.to = e2.from\n",
    "    JOIN nodes n1 ON e1.from = n1.nid\n",
    "    JOIN nodes n2 ON e2.to = n2.nid\n",
    "    WHERE n1.user <> n2.user\n",
    "    GROUP BY e1.to\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 1\n",
    "    \"\"\")\n",
    "# Make sure you only get one result from this!\n",
    "coworkers_company_sdf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 1/1 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "coworkers_company_df = pd.DataFrame(coworkers_company_sdf.collect(),columns=coworkers_company_sdf.columns)\n",
    "\n",
    "if not isinstance(coworkers_company_df, pd.DataFrame) or not 'org' in coworkers_company_df.columns:\n",
    "  raise TypeError(\"Data should be in a DataFrame and organization should be a column\")\n",
    "\n",
    "grader.grade('top_coworkers', coworkers_company_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
